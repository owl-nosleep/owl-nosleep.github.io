---
title: "ã€30å¤©å­¸ç¿’æ–°èªè¨€-Rubyã€‘Day 6ï¼šçˆ¬èŸ²è‡ªå‹•æœå°‹æ•æ„Ÿè³‡è¨Š"
published: 2025-08-15T04:16:46.000Z
description: ""
tags: [Learning, Ruby, 30å¤©å­¸RubyæŒ‘æˆ°]
category: Learning
draft: false
---

## ğŸ’¡ ç‚ºä»€éº¼è¦å¯«çˆ¬èŸ²ï¼Ÿ
å¦‚æœå»çœ‹å¸¸è¦‹çš„ä¸€äº›è³‡å®‰äº‹ä»¶ï¼Œ
æ‡‰è©²æœƒç™¼ç¾å¾ˆå¤šè³‡æ–™å¤–æ´©å…¶å¯¦ä¸æ˜¯è¢«é§­ï¼Œ
è€Œæ˜¯é–‹ç™¼è€…ä¸å°å¿ƒæŠŠæ•æ„Ÿè³‡è¨Šæ”¾åœ¨å…¬é–‹çš„åœ°æ–¹ã€‚

åƒæ˜¯ API key å¯«åœ¨ JavaScript è£¡ã€
æ¸¬è©¦å¸³è™Ÿå¯†ç¢¼æ”¾åœ¨è¨»è§£ã€
æˆ–æ˜¯ Email åœ°å€æ²’æœ‰åšå¥½ä¿è­·ç­‰ç­‰ã€‚

ä»Šå¤©è¦å¯«ä¸€å€‹çˆ¬èŸ²ï¼Œè‡ªå‹•åœ¨ç¶²é ä¸­æœå°‹é€™äº›å¯èƒ½çš„æ•æ„Ÿè³‡è¨Šï¼Œ
å†ç”¨æ­£è¦è¡¨é”å¼ï¼ˆRegexï¼‰ä¾†æ‰¾å‡ºç‰¹å®šæ¨¡å¼çš„è³‡æ–™ã€‚

## ğŸ¯ å¤§ç¶±
- ä½¿ç”¨ `open-uri` æŠ“å–ç¶²é å…§å®¹
- æ’°å¯« Regex æ‰¾å‡º Emailã€API Keyã€å¯†ç¢¼ç­‰
- éè¿´çˆ¬å–å¤šå±¤ç¶²é 
- å°‡çµæœè¼¸å‡ºæˆå ±å‘Š

## ğŸ“š çŸ¥è­˜é»
- `URI.open()` -- é–‹å•Ÿç¶²é ä¸¦è®€å–å…§å®¹
- `/pattern/` -- Ruby çš„æ­£è¦è¡¨é”å¼
- `string.scan(/regex/)` -- æ‰¾å‡ºæ‰€æœ‰ç¬¦åˆçš„å­—ä¸²
- `Set.new` -- ä½¿ç”¨é›†åˆé¿å…é‡è¤‡
- `URI.join()` -- è™•ç†ç›¸å°è·¯å¾‘

## ğŸ’» å¯¦ä½œ
```ruby
require 'open-uri'
require 'nokogiri'
require 'uri'
require 'set'

class SensitiveCrawler
  def initialize(base_url, max_depth = 2)
    @base_url = base_url
    @max_depth = max_depth
    @visited = Set.new
    @findings = {
      emails: Set.new,
      api_keys: Set.new,
      passwords: Set.new,
      tokens: Set.new,
      private_ips: Set.new,
      s3_buckets: Set.new
    }
  end
  
  # å®šç¾©å„ç¨®æ•æ„Ÿè³‡è¨Šçš„ Regex
  def patterns
    {
      emails: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/,
      
      # å¸¸è¦‹çš„ API Key æ ¼å¼
      api_keys: /(?:api[_\-]?key|apikey|api_secret)[\s]*[:=][\s]*["']?([a-zA-Z0-9_-]{20,})["']?/i,
      
      # AWS Access Key
      aws_keys: /AKIA[0-9A-Z]{16}/,
      
      # å¯èƒ½çš„å¯†ç¢¼
      passwords: /(?:password|passwd|pwd)[\s]*[:=][\s]*["']([^"']{4,})["']/i,
      
      # JWT Token
      tokens: /eyJ[A-Za-z0-9_=-]+\.[A-Za-z0-9_=-]+\.?[A-Za-z0-9_.+\/=-]*/,
      
      # ç§æœ‰ IP
      private_ips: /(?:10\.\d{1,3}\.\d{1,3}\.\d{1,3}|172\.(?:1[6-9]|2\d|3[01])\.\d{1,3}\.\d{1,3}|192\.168\.\d{1,3}\.\d{1,3})/,
      
      # S3 Bucket
      s3_buckets: /(?:s3\.amazonaws\.com\/[a-z0-9.-]+|[a-z0-9.-]+\.s3\.amazonaws\.com)/i
    }
  end
  
  def crawl(url = @base_url, depth = 0)
    return if depth > @max_depth
    return if @visited.include?(url)
    
    @visited.add(url)
    puts "Crawling: #{url} (depth: #{depth})"
    
    begin
      html = URI.open(url, 'User-Agent' => 'Mozilla/5.0')
      content = html.read
      
      # æœå°‹æ•æ„Ÿè³‡è¨Š
      search_sensitive_data(content, url)
      
      # å¦‚æœé‚„æ²’åˆ°æœ€å¤§æ·±åº¦ï¼Œç¹¼çºŒçˆ¬å–é€£çµ
      if depth < @max_depth
        doc = Nokogiri::HTML(content)
        links = doc.css('a[href]').map { |link| link['href'] }
        
        links.each do |link|
          next if link.nil? || link.empty?
          
          # è™•ç†ç›¸å°è·¯å¾‘
          begin
            absolute_url = URI.join(url, link).to_s
            # åªçˆ¬å–åŒç¶²åŸŸçš„é€£çµ
            if absolute_url.start_with?(@base_url)
              crawl(absolute_url, depth + 1)
            end
          rescue => e
            # å¿½ç•¥ç„¡æ•ˆçš„ URL
          end
        end
      end
      
    rescue => e
      puts "Error crawling #{url}: #{e.message}"
    end
  end
  
  def search_sensitive_data(content, source_url)
    patterns.each do |type, pattern|
      matches = content.scan(pattern)
      next if matches.empty?
      
      matches.flatten.each do |match|
        next if match.nil? || match.empty?
        
        # æ ¹æ“šé¡å‹å„²å­˜
        case type
        when :emails
          @findings[:emails].add(match) if match.include?('@')
        when :api_keys, :aws_keys
          @findings[:api_keys].add(match)
        when :passwords
          # éæ¿¾æ‰æ˜é¡¯çš„å‡å¯†ç¢¼
          unless match =~ /^(password|example|test|demo|sample)$/i
            @findings[:passwords].add("#{match} (found in: #{source_url})")
          end
        when :tokens
          @findings[:tokens].add(match[0..50] + '...') # åªé¡¯ç¤ºéƒ¨åˆ†
        when :private_ips
          @findings[:private_ips].add(match)
        when :s3_buckets
          @findings[:s3_buckets].add(match)
        end
      end
    end
  end
  
  def report
    puts "\n" + "=" * 60
    puts "SENSITIVE DATA REPORT"
    puts "=" * 60
    
    @findings.each do |type, items|
      next if items.empty?
      
      puts "\n#{type.to_s.upcase.gsub('_', ' ')} (Found: #{items.size})"
      items.first(10).each do |item|
        puts "    - #{item}"
      end
      puts "    ... and #{items.size - 10} more" if items.size > 10
    end
    
    if @findings.values.all?(&:empty?)
      puts "\nNo sensitive data found"
    end
    
    puts "\nTotal pages crawled: #{@visited.size}"
  end
end

# ä½¿ç”¨ç¯„ä¾‹
if __FILE__ == $0
  target = ARGV[0] || 'http://example.com'
  
  puts "Starting sensitive data crawler"
  puts "Target: #{target}"
  puts "Max depth: 2"
  
  crawler = SensitiveCrawler.new(target, 2)
  crawler.crawl
  crawler.report
end

```

## ğŸš€ åŸ·è¡Œæ–¹å¼

1. å»ºç«‹æª”æ¡ˆ `sensitive_crawler.rb`

2. åŸ·è¡ŒæŒ‡ä»¤ï¼ˆå¯ä»¥æŒ‡å®šç›®æ¨™ç¶²ç«™ï¼‰ï¼š
```bash
ruby sensitive_crawler.rb http://target-website.com
```

3. é æœŸè¼¸å‡ºï¼š

```
Starting sensitive data crawler
Target: http://example.com
Max depth: 2

Crawling: http://example.com (depth: 0)
Crawling: http://example.com/about (depth: 1)
Crawling: http://example.com/contact (depth: 1)

============================================================
SENSITIVE DATA REPORT
============================================================

EMAILS (Found: 5)
    - admin@example.com
    - support@example.com
    - test@example.com

API Keys (Found: 2)
    - Example123
    - Example123456

PRIVATE IPS (Found: 1)
    - 192.168.1.100

Total pages crawled: 15
```
![image](https://hackmd.io/_uploads/BJ_BgB7cex.png)
